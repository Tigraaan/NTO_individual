# ultimate_professional_solution.py
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# –î–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
from tqdm import tqdm
import time

class UltimateProfessionalPredictor:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_columns = []
        
    def auto_detect_columns(self, df, df_type):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ —Å —É–º–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""
        column_map = {}
        
        # –í—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –∫–æ–ª–æ–Ω–æ–∫
        user_keywords = ['user', 'id', 'client', 'person', 'customer']
        book_keywords = ['book', 'item', 'product', 'movie', 'article'] 
        rating_keywords = ['rating', 'score', 'target', 'label', 'eval']
        read_keywords = ['read', 'has', 'interaction', 'action']
        
        # –ü–æ–∏—Å–∫ user_id
        for col in df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in user_keywords):
                if 'book' not in col_lower and 'item' not in col_lower:
                    column_map['user_id'] = col
                    break
        else:
            column_map['user_id'] = df.columns[0]  # –ü–µ—Ä–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –ü–æ–∏—Å–∫ book_id
        for col in df.columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in book_keywords):
                column_map['book_id'] = col
                break
        else:
            # –í—Ç–æ—Ä–∞—è –∫–æ–ª–æ–Ω–∫–∞ –∏–ª–∏ –ø–µ—Ä–≤–∞—è –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞
            column_map['book_id'] = df.columns[1] if len(df.columns) > 1 else df.columns[0]
        
        # –î–ª—è train –¥–∞–Ω–Ω—ã—Ö –∏—â–µ–º rating –∏ has_read
        if df_type == 'train':
            for col in df.columns:
                col_lower = col.lower()
                if any(keyword in col_lower for keyword in rating_keywords):
                    column_map['rating'] = col
                    break
            
            for col in df.columns:
                col_lower = col.lower()
                if any(keyword in col_lower for keyword in read_keywords):
                    column_map['has_read'] = col
                    break
        
        return column_map
    
    def load_and_prepare_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        print("üìÇ –ó–ê–ì–†–£–ó–ö–ê –ò –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        files = [
            ('train.csv', '–û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ'),
            ('test.csv', '–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ'),
        ]
        
        data = {}
        
        for filename, description in tqdm(files, desc="–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤"):
            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
                for sep in [';', ',', '\t']:
                    try:
                        df = pd.read_csv(filename, sep=sep)
                        if len(df.columns) > 1:  # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–æ–∫
                            data[filename.replace('.csv', '')] = df
                            print(f"   ‚úÖ {description} –∑–∞–≥—Ä—É–∂–µ–Ω—ã (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å: '{sep}')")
                            break
                    except:
                        continue
                else:
                    print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {filename}")
                    data[filename.replace('.csv', '')] = None
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {e}")
                data[filename.replace('.csv', '')] = None
        
        if data['train'] is None:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å train.csv")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
        print("\nüéØ –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–•...")
        train_columns = self.auto_detect_columns(data['train'], 'train')
        print(f"   Train –∫–æ–ª–æ–Ω–∫–∏: {train_columns}")
        
        if data['test'] is not None:
            test_columns = self.auto_detect_columns(data['test'], 'test')
            print(f"   Test –∫–æ–ª–æ–Ω–∫–∏: {test_columns}")
        
        # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
        data['train'] = data['train'].rename(columns=train_columns)
        if data['test'] is not None:
            data['test'] = data['test'].rename(columns=test_columns)
        
        return data['train'], data['test']
    
    def create_features_with_progress(self, df, is_train=True):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ—Ü–µ—Å—Å–∞"""
        print("\nüîß –°–û–ó–î–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í...")
        
        steps = [
            "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö",
            "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π", 
            "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–Ω–∏–≥",
            "–ò–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏",
            "–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è",
            "–§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞"
        ]
        
        pbar = tqdm(total=len(steps), desc="–ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        try:
            # –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            pbar.set_description("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
            if is_train and 'has_read' in df.columns and 'rating' in df.columns:
                df = df[df['has_read'] == 1].copy()
                print(f"   üìñ –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(df)} –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥")
            time.sleep(0.3)
            pbar.update(1)
            
            # –®–∞–≥ 2: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            pbar.set_description("üë§ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
            if is_train and 'rating' in df.columns:
                self.user_stats = df.groupby('user_id').agg({
                    'rating': ['mean', 'count', 'std', 'min', 'max', 'median']
                }).reset_index()
                self.user_stats.columns = ['user_id', 'user_mean', 'user_count', 'user_std', 'user_min', 'user_max', 'user_median']
                self.global_mean = df['rating'].mean()
                self.global_std = df['rating'].std()
                print(f"   üìà –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(self.user_stats)}")
            time.sleep(0.3)
            pbar.update(1)
            
            # –®–∞–≥ 3: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–Ω–∏–≥
            pbar.set_description("üìö –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–Ω–∏–≥")
            if is_train and 'rating' in df.columns:
                self.book_stats = df.groupby('book_id').agg({
                    'rating': ['mean', 'count', 'std', 'min', 'max', 'median']
                }).reset_index()
                self.book_stats.columns = ['book_id', 'book_mean', 'book_count', 'book_std', 'book_min', 'book_max', 'book_median']
                print(f"   üìä –ö–Ω–∏–≥: {len(self.book_stats)}")
            time.sleep(0.3)
            pbar.update(1)
            
            # –®–∞–≥ 4: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            pbar.set_description("üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö")
            df = df.merge(self.user_stats, on='user_id', how='left')
            df = df.merge(self.book_stats, on='book_id', how='left')
            
            # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
            stats_to_fill = {
                'user_mean': self.global_mean, 'user_count': 1, 'user_std': self.global_std,
                'user_min': 1.0, 'user_max': 10.0, 'user_median': self.global_mean,
                'book_mean': self.global_mean, 'book_count': 1, 'book_std': self.global_std,
                'book_min': 1.0, 'book_max': 10.0, 'book_median': self.global_mean
            }
            
            for col, fill_val in stats_to_fill.items():
                if col in df.columns:
                    df[col] = df[col].fillna(fill_val)
            time.sleep(0.3)
            pbar.update(1)
            
            # –®–∞–≥ 5: –ò–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            pbar.set_description("‚öôÔ∏è –ò–Ω–∂–µ–Ω–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏")
            # User features
            df['user_confidence'] = np.log1p(df['user_count']) / 4.0
            df['user_generosity'] = (df['user_mean'] - self.global_mean) / max(self.global_std, 0.1)
            df['user_consistency'] = 1 / (1 + df['user_std'].fillna(1))
            
            # Book features
            df['book_popularity'] = np.log1p(df['book_count']) / 4.0
            df['book_controversial'] = (df['book_std'] > 2.0).astype(int)
            df['book_consistency'] = 1 / (1 + df['book_std'].fillna(1))
            time.sleep(0.3)
            pbar.update(1)
            
            # –®–∞–≥ 6: –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            pbar.set_description("üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
            # Interaction features
            df['mean_interaction'] = df['user_mean'] * df['book_mean'] / 10.0
            df['confidence_interaction'] = df['user_confidence'] * df['book_popularity']
            df['prediction_baseline'] = 0.6 * df['user_mean'] + 0.4 * df['book_mean']
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_columns = [
                'user_mean', 'user_count', 'user_std', 'user_min', 'user_max', 'user_median',
                'book_mean', 'book_count', 'book_std', 'book_min', 'book_max', 'book_median',
                'user_confidence', 'user_generosity', 'user_consistency',
                'book_popularity', 'book_controversial', 'book_consistency',
                'mean_interaction', 'confidence_interaction', 'prediction_baseline'
            ]
            
            available_features = [f for f in feature_columns if f in df.columns]
            df[available_features] = df[available_features].fillna(0)
            
            if is_train:
                self.feature_columns = available_features
            
            pbar.update(1)
            pbar.close()
            
            print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(self.feature_columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            return df[available_features]
            
        except Exception as e:
            pbar.close()
            raise e
    
    def train_with_detailed_progress(self, X, y):
        """–û–±—É—á–µ–Ω–∏–µ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –≤—ã–≤–æ–¥–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        print("\nüöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô")
        print("=" * 50)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
        
        print(f"üìä –†–ê–ó–ú–ï–†–ù–û–°–¢–ò –î–ê–ù–ù–´–•:")
        print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
        print(f"   –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_val.shape}")
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.scalers['standard'] = StandardScaler()
        X_train_scaled = self.scalers['standard'].fit_transform(X_train)
        X_val_scaled = self.scalers['standard'].transform(X_val)
        
        models_performance = []
        
        # 1. Gradient Boosting —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º
        print("\nüî• –û–ë–£–ß–ï–ù–ò–ï GRADIENT BOOSTING")
        print("   " + "‚îÄ" * 40)
        
        gb_model = GradientBoostingRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6,
            min_samples_split=50,
            min_samples_leaf=20,
            random_state=42,
            verbose=1
        )
        
        print("   üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
        gb_model.fit(X_train_scaled, y_train)
        self.models['gb'] = gb_model
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        train_pred_gb = gb_model.predict(X_train_scaled)
        val_pred_gb = gb_model.predict(X_val_scaled)
        
        train_rmse_gb = np.sqrt(mean_squared_error(y_train, train_pred_gb))
        val_rmse_gb = np.sqrt(mean_squared_error(y_val, val_pred_gb))
        train_mae_gb = mean_absolute_error(y_train, train_pred_gb)
        val_mae_gb = mean_absolute_error(y_val, val_pred_gb)
        
        models_performance.append(('Gradient Boosting', val_rmse_gb, val_mae_gb))
        
        print(f"   üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Gradient Boosting:")
        print(f"     Train RMSE: {train_rmse_gb:.4f} | Val RMSE: {val_rmse_gb:.4f}")
        print(f"     Train MAE:  {train_mae_gb:.4f} | Val MAE:  {val_mae_gb:.4f}")
        
        # 2. Random Forest —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        print("\nüå≥ –û–ë–£–ß–ï–ù–ò–ï RANDOM FOREST")
        print("   " + "‚îÄ" * 40)
        
        rf_model = RandomForestRegressor(
            n_estimators=50,
            max_depth=8,
            min_samples_split=30,
            min_samples_leaf=10,
            random_state=42,
            n_jobs=-1,
            verbose=1
        )
        
        print("   üéØ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–µ—Ä–µ–≤—å–µ–≤...")
        rf_model.fit(X_train, y_train)
        self.models['rf'] = rf_model
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        val_pred_rf = rf_model.predict(X_val)
        val_rmse_rf = np.sqrt(mean_squared_error(y_val, val_pred_rf))
        val_mae_rf = mean_absolute_error(y_val, val_pred_rf)
        
        models_performance.append(('Random Forest', val_rmse_rf, val_mae_rf))
        
        print(f"   üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Random Forest:")
        print(f"     Val RMSE: {val_rmse_rf:.4f} | Val MAE: {val_mae_rf:.4f}")
        
        # 3. Ridge Regression
        print("\nüìê –û–ë–£–ß–ï–ù–ò–ï RIDGE REGRESSION")
        print("   " + "‚îÄ" * 40)
        
        ridge_model = Ridge(alpha=1.0, random_state=42)
        ridge_model.fit(X_train_scaled, y_train)
        self.models['ridge'] = ridge_model
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        val_pred_ridge = ridge_model.predict(X_val_scaled)
        val_rmse_ridge = np.sqrt(mean_squared_error(y_val, val_pred_ridge))
        val_mae_ridge = mean_absolute_error(y_val, val_pred_ridge)
        
        models_performance.append(('Ridge Regression', val_rmse_ridge, val_mae_ridge))
        
        print(f"   üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Ridge Regression:")
        print(f"     Val RMSE: {val_rmse_ridge:.4f} | Val MAE: {val_mae_ridge:.4f}")
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        print("\nüèÜ –ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        print("   " + "=" * 50)
        print(f"   {'–ú–û–î–ï–õ–¨':<20} {'RMSE':<10} {'MAE':<10}")
        print("   " + "‚îÄ" * 50)
        
        for name, rmse, mae in sorted(models_performance, key=lambda x: x[1]):
            print(f"   üéØ {name:<18} {rmse:<10.4f} {mae:<10.4f}")
        
        best_model = min(models_performance, key=lambda x: x[1])
        print(f"\n   üí™ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model[0]}")
        print(f"   üìä –õ—É—á—à–∏–π RMSE: {best_model[1]:.4f}")
        
        return best_model[1]
    
    def predict_ensemble(self, X):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–º –º–æ–¥–µ–ª–µ–π"""
        if len(X) == 0:
            return np.array([self.global_mean] * len(X))
        
        X_scaled = self.scalers['standard'].transform(X)
        
        preds_gb = self.models['gb'].predict(X_scaled)
        preds_rf = self.models['rf'].predict(X)
        preds_ridge = self.models['ridge'].predict(X_scaled)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ
        weights = {'gb': 0.5, 'rf': 0.3, 'ridge': 0.2}
        ensemble_pred = (
            weights['gb'] * preds_gb + 
            weights['rf'] * preds_rf + 
            weights['ridge'] * preds_ridge
        )
        
        return ensemble_pred
    
    def run_ultimate_solution(self):
        """–ó–∞–ø—É—Å–∫ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è"""
        print("üéØ –£–õ–¨–¢–ò–ú–ê–¢–ò–í–ù–û–ï –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï")
        print("üí° –° –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º")
        print("=" * 70)
        
        try:
            # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            train, test = self.load_and_prepare_data()
            
            # 2. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X_train = self.create_features_with_progress(train, is_train=True)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            if 'has_read' in train.columns and 'rating' in train.columns:
                y_train = train[train['has_read'] == 1]['rating']
            elif 'rating' in train.columns:
                y_train = train['rating']
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–æ–≤, —Å–æ–∑–¥–∞–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ
                y_train = pd.Series([self.global_mean] * len(X_train))
                print("   ‚ö†Ô∏è –†–µ–π—Ç–∏–Ω–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            
            print(f"\nüìä –§–ò–ù–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø:")
            print(f"   –ü—Ä–∏–∑–Ω–∞–∫–∏: {X_train.shape}")
            print(f"   –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {len(y_train)}")
            
            # 3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
            best_rmse = self.train_with_detailed_progress(X_train, y_train)
            
            # 4. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–µ
            if test is not None:
                print("\nüéØ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô...")
                X_test = self.create_features_with_progress(test, is_train=False)
                X_test = X_test.fillna(0)
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                predictions = []
                for i in tqdm(range(len(X_test)), desc="–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π", unit="–∑–∞–ø–∏—Å—å"):
                    pred = self.predict_ensemble(X_test.iloc[i:i+1])
                    predictions.append(pred[0])
                    time.sleep(0.001)  # –î–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                
                predictions = np.clip(predictions, 1.0, 10.0)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∞–±–º–∏—Ç–∞
                submission = test[['user_id', 'book_id']].copy()
                submission['rating_predict'] = predictions
                
                submission.to_csv('ultimate_professional_submission.csv', index=False)
                
                print(f"\nüíæ –°–ê–ë–ú–ò–¢ –°–û–•–†–ê–ù–ï–ù: ultimate_professional_submission.csv")
                print(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: RMSE = {best_rmse:.4f}")
                
                return submission
            else:
                raise Exception("–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return None

# –ó–ê–ü–£–°–ö
if __name__ == "__main__":
    predictor = UltimateProfessionalPredictor()
    submission = predictor.run_ultimate_solution()
    
    if submission is not None:
        print(f"\nüéâ –£–õ–¨–¢–ò–ú–ê–¢–ò–í–ù–û–ï –†–ï–®–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –°–û–ó–î–ê–ù–û!")
        print("üì§ –û—Ç–ø—Ä–∞–≤–ª—è–π—Ç–µ: ultimate_professional_submission.csv")
    else:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ")
    
    print("üí™ –£–î–ê–ß–ò –í –°–û–†–ï–í–ù–û–í–ê–ù–ò–ò!")